
# EPFO Member Transfer Analysis Project

## 1. Project Overview
This project provides a suite of tools for analyzing simulated member transfer data between establishments, mimicking patterns observed in Employee Provident Fund Organization (EPFO) data. It allows users to generate sample data, perform various static graph analyses, and explore temporal link prediction using Graph Neural Networks (GNNs).

## 2. Features
The project offers several types of analyses:
*   **Establishment-Level Insights**: Metrics about individual establishments (nodes), such as top sources/destinations of members, net gainers/losers, and centrality measures.
*   **Transfer Pattern Insights**: Analysis of the transfers (edges) themselves, including dominant routes, reciprocal transfers, and patterns based on establishment attributes like industry, location, and size.
*   **Network Structure & Community Insights**: Global network properties like density, community detection using the Louvain algorithm, and identification of bridge establishments connecting different communities.
*   **Temporal Link Prediction**: Utilizes PyTorch Geometric to train a GNN model for predicting future member transfers based on simulated temporal data patterns.

## 3. Project Structure
The project is organized into the following key Python files:

*   `data.py`: Handles data-related tasks.
    *   Generates sample establishment and transfer data.
    *   Loads data from CSV files.
    *   Saves data to CSV (`epfo_establishments_nodes.csv`, `epfo_transfers_edges.csv`).
    *   Exports the graph to GEXF format (`epfo_transfers_graph.gexf`) for visualization in tools like Gephi.
*   `graph_analysis.py`: Contains the core analytical functions that operate on graph data. This includes functions for all establishment, transfer, and network structure insights.
*   `run_analysis.py`: The main command-line interface for running the static graph analyses available in `graph_analysis.py`. It allows users to select data sources, choose specific analyses to perform, and customize parameters.
*   `main.py`: Implements the temporal link prediction component using PyTorch and PyTorch Geometric. It generates its own simulated temporal graph data for this purpose.
*   `requirements.txt`: Lists all Python dependencies required for the project.

## 4. Setup and Installation

1.  **Clone the Repository (if applicable)**:
    If you have downloaded this project as a ZIP, extract it. If it's a Git repository, clone it:
    ```bash
    # git clone <repository_url> # Replace with actual URL if hosted
    # cd <repository_directory>
    ```

2.  **Install Dependencies**:
    It's recommended to use a virtual environment (e.g., `venv` or `conda`) to manage dependencies.
    Install the required packages using:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Notes on PyTorch and PyTorch Geometric**:
    The `requirements.txt` file lists `torch` and `torch-geometric`. The installation of PyTorch Geometric and its dependencies (like `torch-scatter`, `torch-sparse`, etc.) can sometimes be platform-specific or require versions compatible with your PyTorch and CUDA setup.
    If you encounter issues with the `pip install -r requirements.txt` for these packages, it's often best to install them separately by following the official PyTorch Geometric installation instructions. This typically involves a command similar to:
    ```bash
    # Example for a specific PyTorch version and CUDA (adjust as needed):
    # pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-$(python -c "import torch; print(torch.__version__)").html
    ```
    Ensure you have a compatible version of PyTorch installed first. Refer to the [PyTorch Geometric documentation](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) for detailed guidance.

## 5. Data
The project can work with two types of data:

*   **Sample Data**: Generated internally by `data.py` (if run directly) or by `run_analysis.py` when using the `sample` data source. This is useful for quick demonstrations and testing.
    *   Default node file: `epfo_establishments_nodes.csv`
    *   Default edge file: `epfo_transfers_edges.csv`
*   **CSV Data**: You can provide your own data in CSV format. The files should follow the structure of the sample data generated by the script.
    *   **Nodes CSV (`epfo_establishments_nodes.csv`)**: Must contain at least `establishment_id` and other optional attributes like `name`, `industry`, `city`, `size_category`.
    *   **Edges CSV (`epfo_transfers_edges.csv`)**: Must contain `source_establishment_id`, `target_establishment_id`, and `members_transferred`.
*   **GEXF Export**: When data is generated or loaded, `data.py` (if its saving functions are triggered, e.g., by its `if __name__ == "__main__":` block) can save the graph as `epfo_transfers_graph.gexf`. This file can be imported into graph visualization software like [Gephi](https://gephi.org/) to explore the network visually.

## 6. Running Analyses (Static Graph Insights via `run_analysis.py`)
The `run_analysis.py` script is the primary way to perform static graph analyses.

*   **Basic Command (uses sample data and runs no specific analysis by default, just loads graph):**
    ```bash
    python run_analysis.py
    ```

*   **Key Command-Line Arguments**:
    *   `--data_source <source>`: Specify data origin.
        *   `sample` (default): Use built-in sample data.
        *   `csv`: Use data from CSV files. When using `csv`, you can also specify `--nodes_csv <path>` and `--edges_csv <path>` if your files are not named `epfo_establishments_nodes.csv` and `epfo_transfers_edges.csv` or are in a different directory.
    *   `--run_establishment_insights`: Perform establishment-level analyses.
    *   `--run_transfer_insights`: Perform transfer pattern analyses.
    *   `--run_network_insights`: Perform network structure and community detection analyses.
    *   `--run_all_insights`: A shortcut to run all the above insight categories.
    *   `--top_n <number>`: Define the number of items for top-N lists (e.g., top 5 sources). Default is 5.
    *   `--industry_A <name> [--industry_B <name>]`: For industry-specific transfer analysis. If only `industry_A` is given, analyzes intra-industry patterns. If `industry_B` is also given, analyzes inter-industry patterns from A to B.
    *   `--location_A <name> [--location_B <name>]`: For location-specific transfer analysis (uses 'city' attribute).
    *   `--size_A <name> [--size_B <name>]`: For size-category-specific transfer analysis.

*   **Examples**:
    1.  Run all analyses on sample data and show top 10 results where applicable:
        ```bash
        python run_analysis.py --run_all_insights --top_n 10
        ```
    2.  Run transfer pattern analysis for "Information Technology" establishments using sample data:
        ```bash
        python run_analysis.py --run_transfer_insights --industry_A "Information Technology"
        ```
    3.  Run transfer pattern analysis between "Manufacturing" and "Logistics" industries:
        ```bash
        python run_analysis.py --run_transfer_insights --industry_A "Manufacturing" --industry_B "Logistics"
        ```
    4.  Run establishment insights using data from custom CSV files:
        ```bash
        python run_analysis.py --data_source csv --nodes_csv path/to/my_nodes.csv --edges_csv path/to/my_edges.csv --run_establishment_insights
        ```
    5. Run Insights using location
       ```bash
       python run_analysis.py --run_transfer_insights --industry_A "Information Technology" --location_A Bangalore
       ```

## 7. Running Temporal Link Prediction (via `main.py`)
The `main.py` script handles temporal link prediction using a GNN model (typically a GCN-LSTM or similar architecture) built with PyTorch Geometric.

*   **Purpose**: To predict future member transfers (links) in the graph based on patterns observed in a sequence of graph snapshots over time.
*   **Data**: This script generates its own simulated temporal data. It does not use the static CSV files or sample data from `data.py` directly for the temporal modeling part.
*   **Command**:
    ```bash
    python main.py
    ```
*   **Output**: The script will typically output training progress (loss, accuracy/AUC) and evaluation results for the link prediction task.

## 8. Outputs
*   **`run_analysis.py`**: Prints all results (DataFrames, lists, metrics) directly to the console, formatted for readability.
*   **`data.py`**: If run directly (`python data.py`) or if its saving functions are explicitly called, it will generate:
    *   `epfo_establishments_nodes.csv`: CSV file of establishment data.
    *   `epfo_transfers_edges.csv`: CSV file of transfer data.
    *   `epfo_transfers_graph.gexf`: A GEXF file for graph visualization.
*   **`main.py`**: Prints training and evaluation metrics for the temporal link prediction model to the console.

## 9. (Optional) Extending the Analysis
The project is designed to be extensible. New graph analysis functions can be added to `graph_analysis.py`. These functions can then be integrated into `run_analysis.py` by:
1.  Importing the new function in `run_analysis.py`.
2.  Adding relevant command-line arguments if customization is needed.
3.  Calling the function within the appropriate analysis block (e.g., establishment, transfer, network) in `run_analysis.py`.

This allows for easy expansion of the analytical capabilities.

## 8. Advanced GNN Applications (via `run_gnn_applications.py`)
Beyond temporal link prediction, the project now includes a suite of advanced GNN applications leveraging models like Graph Convolutional Networks (GCN), GraphSAGE, and Graph Attention Networks (GAT). These applications are accessible through the `run_gnn_applications.py` script.

The core functionalities include:
*   **Node Classification**: Predict attributes of nodes (e.g., industry, city, size category) based on their features and graph structure.
*   **Node Clustering**: Group similar nodes together based on their learned GNN embeddings using algorithms like K-Means. This can help identify communities or segments of establishments with shared characteristics.
*   **Anomaly Detection**: Identify unusual or anomalous nodes (establishments) using GNN-based autoencoders. Nodes with high reconstruction error are flagged as potential anomalies.

### Running Advanced GNN Applications
The `run_gnn_applications.py` script is the entry point for these tasks.

*   **Key Command-Line Arguments**:
    *   `--gnn_model <model_type>`: Specify the GNN architecture. Choices: `gcn`, `sage`, `gat`.
    *   `--application <app_type>`: Specify the application to run. Choices: `classification`, `clustering`, `anomaly_detection`.
    *   `--nodes_file <path>`: Path to the nodes CSV file (default: `epfo_establishments_nodes.csv`).
    *   `--edges_file <path>`: Path to the edges CSV file (default: `epfo_transfers_edges.csv`).
    *   `--epochs <number>`: Number of training epochs.
    *   `--lr <value>`: Learning rate.
    *   `--hidden_channels <number>`: Number of hidden units in GNN layers.
    *   `--embedding_size <number>`: Dimension of node embeddings output by the encoder part of the models.
    *   `--num_gnn_layers <number>`: Number of layers in the GNN encoder.
    *   `--dropout_rate <value>`: Dropout rate for GNN layers.
    *   `--gat_heads <number>`: Number of attention heads (if using GAT).
    *   **For Classification**:
        *   `--target_label <label>`: The node attribute to predict (e.g., `industry`, `city`, `size_category`).
    *   **For Clustering**:
        *   `--n_clusters <number>`: The number of clusters for K-Means.
    *   **For Anomaly Detection**:
        *   `--decoder_hidden_dims <list_of_numbers>`: List of hidden dimensions for the autoencoder's decoder.
        *   `--anomaly_top_n <number>`: Number of top anomalies to report.

*   **Examples**:

    1.  **Node Classification (Industry Prediction using GCN)**:
        ```bash
        python run_gnn_applications.py --gnn_model gcn --application classification --target_label industry --epochs 50 --lr 0.01
        ```

    2.  **Node Clustering (using GraphSAGE encoder and K-Means)**:
        This first trains a GNN autoencoder with a GraphSAGE encoder to learn embeddings, then clusters these embeddings.
        ```bash
        python run_gnn_applications.py --gnn_model sage --application clustering --n_clusters 5 --epochs 50 --embedding_size 32
        ```
        (Note: The GNN encoder specified by `--gnn_model` is used as the encoder part of an autoencoder to generate embeddings for clustering.)

    3.  **Anomaly Detection (using GAT-based Autoencoder)**:
        ```bash
        python run_gnn_applications.py --gnn_model gat --application anomaly_detection --epochs 100 --embedding_size 32 --decoder_hidden_dims 64 32 --anomaly_top_n 15
        ```

## 9. Outputs
*   **`run_analysis.py`**: Prints all results (DataFrames, lists, metrics) directly to the console, formatted for readability.
*   **`data.py`**: If run directly (`python data.py`) or if its saving functions are explicitly called, it will generate:
    *   `epfo_establishments_nodes.csv`: CSV file of establishment data.
    *   `epfo_transfers_edges.csv`: CSV file of transfer data.
    *   `epfo_transfers_graph.gexf`: A GEXF file for graph visualization.
*   **`main.py`**: Prints training and evaluation metrics for the temporal link prediction model to the console.
*   **`run_gnn_applications.py`**: Prints training progress, evaluation results (e.g., accuracy for classification, silhouette score for clustering), and lists of anomalous nodes to the console.

## 10. (Optional) Extending the Analysis
The project is designed to be extensible. New graph analysis functions can be added to `graph_analysis.py`. These functions can then be integrated into `run_analysis.py` by:
1.  Importing the new function in `run_analysis.py`.
2.  Adding relevant command-line arguments if customization is needed.
3.  Calling the function within the appropriate analysis block (e.g., establishment, transfer, network) in `run_analysis.py`.

This allows for easy expansion of the static graph analytical capabilities. Similarly, new GNN models or applications can be developed in `gnn_applications.py` and integrated into `run_gnn_applications.py`.
